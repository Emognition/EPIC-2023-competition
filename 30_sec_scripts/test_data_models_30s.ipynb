{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST RESULTS (MODEL 30 SEC). This code produces the predictions \n",
    "#for the annotations of valence and arousal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs predictions on the test data for each scenario.\n",
    "\n",
    "Here is a detailed explanation for scenario 2. \n",
    "Scenario 2 uses the Random Forest and SVR_RBF, respectively, both trained on the fizz-emg-sub-vid features. The code performs the following steps:\n",
    "\n",
    "1. Loop through all the folds (0 to 4) for scenario 2.\n",
    "2. Load the arousal and valence models for each fold using joblib.\n",
    "3. Set the test data folder for physiology data and annotations folder for the ground truth annotations.\n",
    "4. Create a folder to save the results.\n",
    "5. Loop through the test physiology files ending with \"_resampled_1hz.csv\".\n",
    "6. Load the corresponding ground truth annotations file for the current test file.\n",
    "7. Preprocess the test data for both arousal and valence by handling NaN values, resampling the data to match the annotations length, and selecting the fizz-emg-sub-vid features.\n",
    "8. Use the loaded arousal and valence models to predict arousal and valence values for the test data.\n",
    "9. Add the predicted arousal and valence values to the ground truth annotations DataFrame.\n",
    "10. Save the resulting DataFrame with the predicted arousal and valence values to the specified folder.\n",
    "\n",
    "In summary, this code snippet predicts arousal and valence values for the test data in each scenario using the best models for valence and arousal and saves the predictions in the specified folder.\n",
    "\n",
    "Nota Bene:\n",
    "The code starts by specifying the feature of interest for each models and copies the pipeline helper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns of interest\n",
    "\n",
    "fizz_columns_of_interest = [\n",
    "    'PPG_Clean', 'PPG_Rate', 'PPG_Peaks',\n",
    "    'ECG_Rate','ECG_Quality',\n",
    "    'EDA_Clean', 'EDA_Tonic', 'EDA_Phasic', 'SCR_Onsets', 'SCR_Peaks','SCR_Height', 'SCR_Amplitude',\n",
    "    'RSP_Amplitude', 'RSP_Rate',\n",
    "    'corrugator_EMG_Clean',\n",
    "    'corrugator_EMG_Amplitude', 'corrugator_EMG_Activity',\n",
    "    'corrugator_EMG_Onsets', 'corrugator_EMG_Offsets',\n",
    "    'trapezius_EMG_Clean', 'trapezius_EMG_Amplitude',\n",
    "    'trapezius_EMG_Activity', 'trapezius_EMG_Onsets',\n",
    "    'trapezius_EMG_Offsets', 'zygomaticus_EMG_Clean',\n",
    "    'zygomaticus_EMG_Amplitude', 'zygomaticus_EMG_Activity',\n",
    "    'zygomaticus_EMG_Onsets', 'zygomaticus_EMG_Offsets',\n",
    "    'RSP_RVT', 'RSP_Phase', 'RSP_Phase_Completion',\n",
    "    'RSP_Symmetry_PeakTrough', 'RSP_Symmetry_RiseDecay','skt',\n",
    "    #'sub_num','vid_num',\n",
    "]\n",
    "\n",
    "fizz_columns_of_interest = [ \n",
    "    'PPG_Rate', \n",
    "    'ECG_Rate','ECG_Quality',\n",
    "    'EDA_Clean', 'EDA_Tonic', 'EDA_Phasic', 'SCR_Onsets', 'SCR_Peaks','SCR_Height', 'SCR_Amplitude',\n",
    "    'RSP_Amplitude', 'RSP_Rate',\n",
    "    'corrugator_EMG_Clean',\n",
    "    'corrugator_EMG_Amplitude', 'corrugator_EMG_Activity',\n",
    "    'corrugator_EMG_Onsets', 'corrugator_EMG_Offsets',\n",
    "    'trapezius_EMG_Clean', 'trapezius_EMG_Amplitude',\n",
    "    'trapezius_EMG_Activity', 'trapezius_EMG_Onsets',\n",
    "    'trapezius_EMG_Offsets', 'zygomaticus_EMG_Clean',\n",
    "    'zygomaticus_EMG_Amplitude', 'zygomaticus_EMG_Activity',\n",
    "    'zygomaticus_EMG_Onsets', 'zygomaticus_EMG_Offsets',\n",
    "    'RSP_RVT', 'RSP_Phase', 'RSP_Phase_Completion',\n",
    "    'RSP_Symmetry_PeakTrough', 'RSP_Symmetry_RiseDecay','skt',\n",
    "    # 'sub_num',\n",
    "    # 'vid_num',\n",
    "]\n",
    "\n",
    "fizz_columns_of_interest_no_emg = [ \n",
    "    'PPG_Rate', \n",
    "    'ECG_Rate','ECG_Quality',\n",
    "    'EDA_Clean', 'EDA_Tonic', 'EDA_Phasic', 'SCR_Onsets', 'SCR_Peaks','SCR_Height', 'SCR_Amplitude',\n",
    "    'RSP_Amplitude', 'RSP_Rate',\n",
    "    'RSP_RVT', 'RSP_Phase', 'RSP_Phase_Completion',\n",
    "    'RSP_Symmetry_PeakTrough', 'RSP_Symmetry_RiseDecay','skt',\n",
    "    # 'sub_num',\n",
    "    # 'vid_num',\n",
    "]\n",
    "\n",
    "fizz_columns_of_interest_no_emg_sub_vid = [ \n",
    "    'PPG_Rate', \n",
    "    'ECG_Rate','ECG_Quality',\n",
    "    'EDA_Clean', 'EDA_Tonic', 'EDA_Phasic', 'SCR_Onsets', 'SCR_Peaks','SCR_Height', 'SCR_Amplitude',\n",
    "    'RSP_Amplitude', 'RSP_Rate',\n",
    "    'RSP_RVT', 'RSP_Phase', 'RSP_Phase_Completion',\n",
    "    'RSP_Symmetry_PeakTrough', 'RSP_Symmetry_RiseDecay','skt',\n",
    "    'sub_num',\n",
    "    'vid_num',\n",
    "]\n",
    "\n",
    "fizz_columns_of_interest_sub_vid = [ \n",
    "    'PPG_Rate', \n",
    "    'ECG_Rate','ECG_Quality',\n",
    "    'EDA_Clean', 'EDA_Tonic', 'EDA_Phasic', 'SCR_Onsets', 'SCR_Peaks','SCR_Height', 'SCR_Amplitude',\n",
    "    'RSP_Amplitude', 'RSP_Rate',\n",
    "    'corrugator_EMG_Clean',\n",
    "    'corrugator_EMG_Amplitude', 'corrugator_EMG_Activity',\n",
    "    'corrugator_EMG_Onsets', 'corrugator_EMG_Offsets',\n",
    "    'trapezius_EMG_Clean', 'trapezius_EMG_Amplitude',\n",
    "    'trapezius_EMG_Activity', 'trapezius_EMG_Onsets',\n",
    "    'trapezius_EMG_Offsets', 'zygomaticus_EMG_Clean',\n",
    "    'zygomaticus_EMG_Amplitude', 'zygomaticus_EMG_Activity',\n",
    "    'zygomaticus_EMG_Onsets', 'zygomaticus_EMG_Offsets',\n",
    "    'RSP_RVT', 'RSP_Phase', 'RSP_Phase_Completion',\n",
    "    'RSP_Symmetry_PeakTrough', 'RSP_Symmetry_RiseDecay','skt',\n",
    "    'sub_num',\n",
    "    'vid_num',\n",
    "]\n",
    "\n",
    "zipped_xs = list(zip(['fizz','fizz-emg','fizz-emg_sub_vid','fizz_sub_vid'],[fizz_columns_of_interest,\n",
    "     fizz_columns_of_interest_no_emg,\n",
    "     fizz_columns_of_interest_no_emg_sub_vid,\n",
    "     fizz_columns_of_interest_sub_vid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copying the pipeline helper\n",
    "\n",
    "\n",
    "import glob,os\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.signal import resample\n",
    "\n",
    "\n",
    "def get_file_paths(subjects, video_numbers, x_folder, y_folder, file_type = '1hz_zscored'):\n",
    "    x_files = []\n",
    "    y_files = []\n",
    "\n",
    "    for subject in subjects:\n",
    "        for video in video_numbers:\n",
    "            x_pattern = os.path.join(x_folder,\n",
    "                                     f\"sub_{int(subject)}_vid_{int(video)}_resampled_{file_type}.csv\")\n",
    "            y_pattern = os.path.join(y_folder, f\"sub_{int(subject)}_vid_{int(video)}.csv\")\n",
    "            #print(x_pattern)\n",
    "\n",
    "            x_files.extend(glob.glob(x_pattern))\n",
    "            y_files.extend(glob.glob(y_pattern))\n",
    "\n",
    "    return x_files, y_files\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EmognitionDataset(Dataset):\n",
    "    def __init__(self, x_files, y_files, x_columns=None, y_columns=None, downsampling_factor_x=1,downsampling_factor_y=1, upsample_factor_x=1, upsample_factor_y=1):\n",
    "        self.x_data = []\n",
    "        self.y_data = []  \n",
    "        self.sub_num_data = []\n",
    "        self.vid_num_data = []\n",
    "\n",
    "        for file in x_files:\n",
    "            x_df = pd.read_csv(file)\n",
    "            x_array = x_df[x_columns].values[::downsampling_factor_x]\n",
    "            if x_array.shape[0] == 1:\n",
    "                x_array = np.nan_to_num(x_array)\n",
    "            \n",
    "            if upsample_factor_x > 1:\n",
    "                x_array = self.upsample(x_array, upsample_factor_x)\n",
    "            \n",
    "            self.x_data.append(x_array)\n",
    "\n",
    "            sub_num_array = x_df['sub_num'].values[::downsampling_factor_x]\n",
    "            vid_num_array = x_df['vid_num'].values[::downsampling_factor_x]\n",
    "            \n",
    "            if upsample_factor_x > 1:\n",
    "                new_num_samples = sub_num_array.shape[0] * upsample_factor_x\n",
    "                sub_num_array = resample(sub_num_array, new_num_samples)\n",
    "                vid_num_array = resample(vid_num_array, new_num_samples)\n",
    "            \n",
    "            self.sub_num_data.append(sub_num_array)\n",
    "            self.vid_num_data.append(vid_num_array)\n",
    "     \n",
    "\n",
    "        for file in y_files:\n",
    "            y_df = pd.read_csv(file, usecols=y_columns)\n",
    "            y_array = y_df.values[::downsampling_factor_y]\n",
    "            \n",
    "            if upsample_factor_y > 1:\n",
    "                new_num_samples = y_array.shape[0] * upsample_factor_y\n",
    "                y_array = resample(y_array.squeeze(), new_num_samples)\n",
    "            \n",
    "            self.y_data.append(y_array)\n",
    "            \n",
    "            \n",
    "        for i in range(len(self.x_data)):\n",
    "            # make sure they are the same length between x_file and y_files\n",
    "            min_len = min(len(self.x_data[i]), len(self.y_data[i]))\n",
    "            self.x_data[i] = self.x_data[i][:min_len]\n",
    "            self.y_data[i] = self.y_data[i][:min_len]\n",
    "            self.sub_num_data[i] = self.sub_num_data[i][:min_len]\n",
    "            self.vid_num_data[i] = self.vid_num_data[i][:min_len]\n",
    "            \n",
    "#             if upsample_factor_x > 1:\n",
    "#                 print(self.x_data[i].shape)\n",
    "#                 print(self.y_data[i].shape)\n",
    "#                 print(self.sub_num_data[i].shape)\n",
    "#                 print(self.vid_num_data[i].shape)\n",
    "            \n",
    "            # Combine x and y data, drop rows with NaN values\n",
    "            combined_df = pd.concat([pd.DataFrame(self.x_data[i], columns=x_columns),\n",
    "                         pd.DataFrame(self.y_data[i], columns=y_columns),\n",
    "                         pd.DataFrame(self.sub_num_data[i], columns=['sub_num_dataset']),\n",
    "                         pd.DataFrame(self.vid_num_data[i], columns=['vid_num_dataset'])], axis=1)\n",
    "\n",
    "            # Drop rows with NaN values\n",
    "            combined_df = combined_df.dropna()\n",
    "\n",
    "            # Extract cleaned x, y, sub_num, and vid_num data\n",
    "            self.x_data[i] = combined_df[x_columns].values\n",
    "            self.y_data[i] = combined_df[y_columns].values\n",
    "            self.sub_num_data[i] = combined_df['sub_num_dataset'].values\n",
    "            self.vid_num_data[i] = combined_df['vid_num_dataset'].values\n",
    "  \n",
    "    def upsample(self, x_array, upsample_factor_x):\n",
    "        if x_array.shape[0] > 1:\n",
    "            new_num_samples = x_array.shape[0] * upsample_factor_x\n",
    "            col_mean = np.nanmean(x_array, axis=0)\n",
    "            inds = np.where(np.isnan(x_array))\n",
    "            x_array[inds] = np.take(col_mean, inds[1])\n",
    "            x_array = resample(x_array.squeeze(), new_num_samples)\n",
    "        elif x_array.shape[0] == 1 or x_array.ndim == 1:\n",
    "            new_num_samples = x_array.shape[0] * upsample_factor_x\n",
    "            col_mean = np.nanmean(x_array, axis=0)\n",
    "            inds = np.where(np.isnan(x_array))\n",
    "            x_array[inds] = np.take(col_mean, inds[1])\n",
    "            x_array = resample(x_array, new_num_samples)\n",
    "        return x_array\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x_data[index]\n",
    "        y = self.y_data[index]\n",
    "        sub_num = self.sub_num_data[index]\n",
    "        vid_num = self.vid_num_data[index]\n",
    "\n",
    "        return x, y, sub_num, vid_num\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "from math import sqrt\n",
    "from scipy.signal import resample\n",
    "\n",
    "\n",
    "def upsample_predictions(predictions, factor):\n",
    "    \"\"\"\n",
    "    Upsample the predictions by a given factor using resample function.\n",
    "    \n",
    "    :param predictions: np.array of predictions\n",
    "    :param factor: int, factor to upsample the predictions\n",
    "    :return: np.array of upsampled predictions\n",
    "    \"\"\"\n",
    "    new_num_samples = predictions.shape[0] * factor\n",
    "    upsampled_predictions = resample(predictions.squeeze(), new_num_samples)\n",
    "    \n",
    "    return upsampled_predictions.reshape(-1, 1)\n",
    "\n",
    "def TestModel(model, fold_data, x_columns, y_columns, save_file_name, \n",
    "                       \n",
    "                      verbose = True):\n",
    "    '''\n",
    "    which_features: x column names\n",
    "    which_y: y column name\n",
    "    save_file_name: descriptive name for the specific version of this model.\n",
    "    e.g., LinearReg_arousal_\n",
    "    \n",
    "    '''\n",
    "    test_res = []\n",
    "    all_pred_y = []\n",
    "    all_sub_num = []\n",
    "    all_vid_num = []\n",
    "    for f, fold in enumerate(fold_data):\n",
    "        test_x_files, test_y_files = fold[1]\n",
    "\n",
    "        \n",
    "        test_dataset = EmognitionDataset(\n",
    "            test_x_files, test_y_files, x_columns=x_columns, y_columns=y_columns,\n",
    "         upsample_factor_x=20, upsample_factor_y=1\n",
    "        )\n",
    "    \n",
    "\n",
    "#         train_x = np.vstack([train_dataset[i][0] for i in range(len(train_dataset))])\n",
    "#         train_y = np.vstack([train_dataset[i][1] for i in range(len(train_dataset))])\n",
    "\n",
    "        test_x = np.vstack([test_dataset[i][0] for i in range(len(test_dataset))])\n",
    "        test_y = np.vstack([test_dataset[i][1] for i in range(len(test_dataset))])\n",
    "        #print([test_dataset[i][2] for i in range(len(test_dataset))])\n",
    "        test_sub = np.hstack([test_dataset[i][2] for i in range(len(test_dataset))])\n",
    "        test_vid = np.hstack([test_dataset[i][3] for i in range(len(test_dataset))])\n",
    "        \n",
    "#         s = 0\n",
    "#         for i in range(len(test_dataset)):\n",
    "#             s += test_dataset[i][0].shape[0]\n",
    "#             print(s)\n",
    "#         print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)\n",
    "        \n",
    "\n",
    "       # Make predictions\n",
    "        pred_y = model.predict(test_x).reshape(-1, 1)\n",
    "        \n",
    "        # Upsample predictions\n",
    "#         upsampled_pred_y = upsample_predictions(pred_y, downsampling_factor_y)\n",
    "\n",
    "        # Calculate metrics with upsampled predictions\n",
    "        rmse = sqrt(mean_squared_error(test_y, pred_y))\n",
    "        test_res.append(rmse)\n",
    "        r2 = r2_score(test_y, pred_y)\n",
    "        \n",
    "        \n",
    "        if verbose:\n",
    "            print(f'---------------------- fold {f+1} ---------------------------')\n",
    "            print(f'rmse = {rmse:.3f}')\n",
    "            print(f'r2 score = {r2:.3f}')\n",
    "\n",
    "        # save: \n",
    "        all_pred_y.append(pred_y)\n",
    "        all_sub_num.append(test_sub)\n",
    "        all_vid_num.append(test_vid)\n",
    "\n",
    "  #  return all_pred_y\n",
    "    all_pred_y_concat = np.vstack(all_pred_y)\n",
    "    all_sub_num_concat = np.hstack(all_sub_num)\n",
    "    all_vid_num_concat = np.hstack(all_vid_num)\n",
    "\n",
    "    all_pred_y_df = pd.DataFrame({'sub': all_sub_num_concat, 'vid': all_vid_num_concat, 'pred_y': all_pred_y_concat[:,0]})\n",
    "    all_pred_y_df.to_csv(save_file_name + '.csv', index=False)\n",
    "\n",
    "\n",
    "    joblib.dump(model, save_file_name + '.joblib')\n",
    "    return test_res, all_pred_y_df, model\n",
    "\n",
    "def train_model_no_eval(model, \n",
    "                        train_x_files,\n",
    "                        train_y_files,\n",
    "                        x_columns, \n",
    "                        y_columns, \n",
    "                        save_file_name, \n",
    "                        downsampling_factor_y=20, \n",
    "                        downsampling_factor_x=1, \n",
    "                        verbose = True):\n",
    "    '''\n",
    "    which_features: x column names\n",
    "    which_y: y column name\n",
    "    save_file_name: descriptive name for the specific version of this model.\n",
    "    e.g., LinearReg_arousal_\n",
    "    \n",
    "    '''\n",
    "    train_x_files, train_y_files\n",
    "\n",
    "    train_dataset = EmognitionDataset(\n",
    "        train_x_files, \n",
    "        train_y_files,\n",
    "        x_columns=x_columns,\n",
    "        y_columns=y_columns,\n",
    "        downsampling_factor_y=downsampling_factor_y,\n",
    "        downsampling_factor_x=downsampling_factor_x\n",
    "    )\n",
    "\n",
    "    train_x = np.vstack([train_dataset[i][0] for i in range(len(train_dataset))])\n",
    "    train_y = np.vstack([train_dataset[i][1] for i in range(len(train_dataset))])\n",
    "    model.fit(train_x, train_y)\n",
    "    joblib.dump(model, save_file_name + '.joblib')\n",
    "    return model\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scenario 2\n",
    "\n",
    "#random forest (fizz_emg) --> arousal\n",
    "#SVR_RBF (fizz_emg)--> valence\n",
    "\n",
    "# Scenario 2 valence elastic_net fizz-emg_sub_vid\n",
    "# Scenario2 arousal rf fizz_sub-emg_sub_vid\n",
    "\n",
    "for fold_num in range(0,5):        \n",
    "    arousal_model_name = f'/work/abslab/emognition_2023_challenge/results/results_30s/scenario_2/fold_{fold_num}/random_forest_default_arousal_fizz-emg_sub_vid.joblib'\n",
    "    valence_model_name = f'/work/abslab/emognition_2023_challenge/results/results_30s/scenario_2/fold_{fold_num}/Elastic_Net_valence_fizz-emg_sub_vid.joblib'\n",
    "    \n",
    "    arousal_model = joblib.load(arousal_model_name)\n",
    "    valence_model = joblib.load(valence_model_name)\n",
    "    \n",
    "    test_data_folder =  f'/work/abslab/emognition_2023_challenge/file_prep_30sec/scenario_2/fold_{fold_num}/test/physiology/'\n",
    "    annotations_folder = f'/work/abslab/emognition_2023_challenge/data/scenario_2/fold_{fold_num}/test/annotations/'\n",
    "    \n",
    "    save_data_folder = f'/work/abslab/emognition_2023_challenge/results/results_30s/scenario_2/fold_{fold_num}/test/'\n",
    "    \n",
    "    for file in os.listdir(test_data_folder):\n",
    "        if file.endswith(\"_resampled_1hz.csv\"):\n",
    "            print(file)\n",
    "            \n",
    "            annotations_file_name = file[:-len(\"_resampled_1hz.csv\")] + \".csv\"\n",
    "            annotations_file_ = os.path.join(annotations_folder, annotations_file_name)\n",
    "            annotations_data = pd.read_csv(annotations_file_, index_col=0)\n",
    "            annotations_len = annotations_data.shape[0]\n",
    "            \n",
    "            file_ = os.path.join(test_data_folder, file)\n",
    "            \n",
    "            test_data_arousal = pd.read_csv(file_, index_col=0)\n",
    "            test_data_arousal = test_data_arousal[fizz_columns_of_interest_no_emg_sub_vid]\n",
    "            if test_data_arousal.shape[0] == 1:\n",
    "                test_data_arousal = np.nan_to_num(test_data_arousal)\n",
    "            new_num_samples = annotations_len\n",
    "            col_mean = np.nanmean(test_data_arousal, axis=0)\n",
    "            inds = np.where(np.isnan(test_data_arousal))\n",
    "            test_data_arousal[inds] = np.take(col_mean, inds[1])\n",
    "            test_data_arousal = resample(test_data_arousal, new_num_samples)\n",
    "            \n",
    "            test_data_valence = pd.read_csv(file_, index_col=0)\n",
    "            test_data_valence = test_data_valence[fizz_columns_of_interest_no_emg_sub_vid]\n",
    "            if test_data_valence.shape[0] == 1:\n",
    "                test_data_valence = np.nan_to_num(test_data_valence)\n",
    "            new_num_samples = annotations_len\n",
    "            col_mean = np.nanmean(test_data_valence, axis=0)\n",
    "            inds = np.where(np.isnan(test_data_valence))\n",
    "            test_data_valence[inds] = np.take(col_mean, inds[1])\n",
    "            test_data_valence = resample(test_data_valence, new_num_samples)\n",
    "            \n",
    "            arousal_pred_y = arousal_model.predict(test_data_arousal).reshape(-1, 1)\n",
    "            valence_pred_y = valence_model.predict(test_data_valence).reshape(-1, 1)\n",
    "            \n",
    "            annotations_data['valence'] = valence_pred_y\n",
    "            annotations_data['arousal'] = arousal_pred_y\n",
    "            \n",
    "            save_file_ = os.path.join(save_data_folder, annotations_file_name)\n",
    "            annotations_data.to_csv(save_file_)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scenario 3\n",
    "#random forest (fizz_sub_video)--> arousal\n",
    "#SVR_RBF (fizz_emg)--> valence\n",
    "\n",
    "\n",
    "for fold_num in range(0,4):        \n",
    "    arousal_model_name = f'/work/abslab/emognition_2023_challenge/results/results_30s/scenario_3/fold_{fold_num}/SVR_RBF_arousal_fizz.joblib'\n",
    "    #valence_model_name = f'/work/abslab/emognition_2023_challenge/results/results_30s/scenario_3/fold_{fold_num}/SVR_RBF_valence_fizz-emg.joblib'\n",
    "    \n",
    "    arousal_model = joblib.load(arousal_model_name)\n",
    "    #valence_model = joblib.load(valence_model_name)\n",
    "    \n",
    "    test_data_folder =  f'/work/abslab/emognition_2023_challenge/file_prep_30sec/scenario_3/fold_{fold_num}/test/physiology/'\n",
    "    annotations_folder = f'/work/abslab/emognition_2023_challenge/results/final_submission_alessia/results/scenario_3/fold_{fold_num}/test/annotations/'\n",
    "    \n",
    "    save_data_folder = f'/work/abslab/emognition_2023_challenge/results/final_submission_alessia/results/scenario_3/fold_{fold_num}/test/annotations/'\n",
    "    \n",
    "    for file in os.listdir(test_data_folder):\n",
    "        if file.endswith(\"_resampled_1hz.csv\"):\n",
    "            print(file)\n",
    "            \n",
    "            annotations_file_name = file[:-len(\"_resampled_1hz.csv\")] + \".csv\"\n",
    "            annotations_file_ = os.path.join(annotations_folder, annotations_file_name)\n",
    "            annotations_data = pd.read_csv(annotations_file_, index_col=0)\n",
    "            annotations_len = annotations_data.shape[0]\n",
    "            \n",
    "            file_ = os.path.join(test_data_folder, file)\n",
    "            \n",
    "            test_data_arousal = pd.read_csv(file_, index_col=0)\n",
    "            test_data_arousal = test_data_arousal[fizz_columns_of_interest]\n",
    "            if test_data_arousal.shape[0] == 1:\n",
    "                test_data_arousal = np.nan_to_num(test_data_arousal)\n",
    "            new_num_samples = annotations_len\n",
    "            col_mean = np.nanmean(test_data_arousal, axis=0)\n",
    "            inds = np.where(np.isnan(test_data_arousal))\n",
    "            test_data_arousal[inds] = np.take(col_mean, inds[1])\n",
    "            test_data_arousal = resample(test_data_arousal, new_num_samples)\n",
    "            \n",
    "#             test_data_valence = pd.read_csv(file_, index_col=0)\n",
    "#             test_data_valence = test_data_valence[fizz_columns_of_interest_no_emg]\n",
    "#             if test_data_valence.shape[0] == 1:\n",
    "#                 test_data_valence = np.nan_to_num(test_data_valence)\n",
    "#             new_num_samples = annotations_len\n",
    "#             col_mean = np.nanmean(test_data_valence, axis=0)\n",
    "#             inds = np.where(np.isnan(test_data_valence))\n",
    "#             test_data_valence[inds] = np.take(col_mean, inds[1])\n",
    "#             test_data_valence = resample(test_data_valence, new_num_samples)\n",
    "            \n",
    "            arousal_pred_y = arousal_model.predict(test_data_arousal).reshape(-1, 1)\n",
    "#             valence_pred_y = valence_model.predict(test_data_valence).reshape(-1, 1)\n",
    "            \n",
    "#             annotations_data['valence'] = valence_pred_y\n",
    "            annotations_data['arousal'] = arousal_pred_y\n",
    "            \n",
    "            save_file_ = os.path.join(save_data_folder, annotations_file_name)\n",
    "            annotations_data.to_csv(save_file_)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scenario 4\n",
    "\n",
    "#random forest (fizz_emg_sub_video)---> arousal\n",
    "# SVR (fizz) --> valence\n",
    "\n",
    "\n",
    "\n",
    "for fold_num in range(0,3):        \n",
    "    arousal_model_name = f'/work/abslab/emognition_2023_challenge/results/results_30s/scenario_4/fold_{fold_num}/SVR_RBF_arousal_fizz.joblib'\n",
    "#     valence_model_name = f'/work/abslab/emognition_2023_challenge/results/results_30s/scenario_4/fold_{fold_num}/SVR_RBF_valence_fizz.joblib'\n",
    "    \n",
    "    arousal_model = joblib.load(arousal_model_name)\n",
    "#     valence_model = joblib.load(valence_model_name)\n",
    "    \n",
    "    test_data_folder =  f'/work/abslab/emognition_2023_challenge/file_prep_30sec/scenario_4/fold_{fold_num}/test/physiology/'\n",
    "    annotations_folder = f'/work/abslab/emognition_2023_challenge/results/final_submission_alessia/results/scenario_4/fold_{fold_num}/test/annotations/'\n",
    "    \n",
    "    save_data_folder = f'/work/abslab/emognition_2023_challenge/results/final_submission_alessia/results/scenario_4/fold_{fold_num}/test/annotations/'\n",
    "     \n",
    "    for file in os.listdir(test_data_folder):\n",
    "        if file.endswith(\"_resampled_1hz.csv\"):\n",
    "            print(file)\n",
    "            \n",
    "            annotations_file_name = file[:-len(\"_resampled_1hz.csv\")] + \".csv\"\n",
    "            annotations_file_ = os.path.join(annotations_folder, annotations_file_name)\n",
    "            annotations_data = pd.read_csv(annotations_file_, index_col=0)\n",
    "            annotations_len = annotations_data.shape[0]\n",
    "            \n",
    "            file_ = os.path.join(test_data_folder, file)\n",
    "            \n",
    "            test_data_arousal = pd.read_csv(file_, index_col=0)\n",
    "            test_data_arousal = test_data_arousal[fizz_columns_of_interest]\n",
    "            if test_data_arousal.shape[0] == 1:\n",
    "                test_data_arousal = np.nan_to_num(test_data_arousal)\n",
    "            new_num_samples = annotations_len\n",
    "            col_mean = np.nanmean(test_data_arousal, axis=0)\n",
    "            inds = np.where(np.isnan(test_data_arousal))\n",
    "            test_data_arousal[inds] = np.take(col_mean, inds[1])\n",
    "            test_data_arousal = resample(test_data_arousal, new_num_samples)\n",
    "            \n",
    "#             test_data_valence = pd.read_csv(file_, index_col=0)\n",
    "#             test_data_valence = test_data_valence[fizz_columns_of_interest]\n",
    "#             if test_data_valence.shape[0] == 1:\n",
    "#                 test_data_valence = np.nan_to_num(test_data_valence)\n",
    "#             new_num_samples = annotations_len\n",
    "#             col_mean = np.nanmean(test_data_valence, axis=0)\n",
    "#             inds = np.where(np.isnan(test_data_valence))\n",
    "#             test_data_valence[inds] = np.take(col_mean, inds[1])\n",
    "#             test_data_valence = resample(test_data_valence, new_num_samples)\n",
    "            \n",
    "            arousal_pred_y = arousal_model.predict(test_data_arousal).reshape(-1, 1)\n",
    "#             valence_pred_y = valence_model.predict(test_data_valence).reshape(-1, 1)\n",
    "            \n",
    "#             annotations_data['valence'] = valence_pred_y\n",
    "            annotations_data['arousal'] = arousal_pred_y\n",
    "            \n",
    "            save_file_ = os.path.join(save_data_folder, annotations_file_name)\n",
    "            annotations_data.to_csv(save_file_)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
