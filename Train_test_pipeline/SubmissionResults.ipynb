{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53e7bdb9-ee47-4ddc-9434-276044d7d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import pipeline_helper\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn import ensemble\n",
    "import os\n",
    "from scipy.signal import resample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88481f61-b834-446d-8732-64debad9fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_to_answer_shape(test_pred,\n",
    "                             annot_len,\n",
    "                             sampling_rate=1,\n",
    "                             upsampling_factor=20,\n",
    "                             lead_in_period_seconds=10,\n",
    "                            ):\n",
    "    test_pred = test_pred.reshape(1,-1)[0]\n",
    "    duration = test_pred.shape[0]/sampling_rate\n",
    "    upsampling_factor = 20\n",
    "    new_sampling_rate = sampling_rate * upsampling_factor\n",
    "    new_num_samples = duration * new_sampling_rate\n",
    "    upsampled_signal = resample(test_pred, int(new_num_samples))\n",
    "    start_test_period = lead_in_period_seconds*upsampling_factor\n",
    "    test_period_test_pred_upsampled = upsampled_signal[start_test_period:(start_test_period+annot_len)]\n",
    "    return test_period_test_pred_upsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d52aad5-59e2-45cb-946e-8b8507881bd6",
   "metadata": {},
   "source": [
    "# Scenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c51c7287-1dd1-4a50-a0d8-91e068c38fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcveigh.k/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcveigh.k/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcveigh.k/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcveigh.k/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcveigh.k/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "scenario = 2\n",
    "y_columns=['arousal']\n",
    "for fold in range(5):\n",
    "    print(fold)\n",
    "    train_y_files = glob.glob(\n",
    "        f'/work/abslab/emognition_2023_challenge/data/scenario_{scenario}/fold_{fold}/train/annotations/*')\n",
    "    train_x_files = glob.glob(\n",
    "        f'/work/abslab/emognition_2023_challenge/data/scenario_{scenario}/fold_{fold}/train/physiology/*1hz_zscored.csv')\n",
    "    model = svm.SVR()\n",
    "    x_columns = [ \n",
    "        'PPG_Rate', \n",
    "        'ECG_Rate','ECG_Quality','PPG_Peak_Height',\n",
    "        'EDA_Clean', 'EDA_Tonic', 'EDA_Phasic', 'SCR_Onsets', 'SCR_Peaks','SCR_Height', 'SCR_Amplitude',\n",
    "        'RSP_Amplitude', 'RSP_Rate',\n",
    "        'corrugator_EMG_Clean',\n",
    "        'corrugator_EMG_Amplitude', 'corrugator_EMG_Activity',\n",
    "        'corrugator_EMG_Onsets', 'corrugator_EMG_Offsets', 'trapezius_EMG_Raw',\n",
    "        'trapezius_EMG_Clean', 'trapezius_EMG_Amplitude',\n",
    "        'trapezius_EMG_Activity', 'trapezius_EMG_Onsets',\n",
    "        'trapezius_EMG_Offsets', 'zygomaticus_EMG_Raw', 'zygomaticus_EMG_Clean',\n",
    "        'zygomaticus_EMG_Amplitude', 'zygomaticus_EMG_Activity',\n",
    "        'zygomaticus_EMG_Onsets', 'zygomaticus_EMG_Offsets',\n",
    "        'RSP_RVT', 'RSP_Phase', 'RSP_Phase_Completion',\n",
    "        'RSP_Symmetry_PeakTrough', 'RSP_Symmetry_RiseDecay','skt',\n",
    "        'sub_num',\n",
    "        'vid_num',\n",
    "    ]\n",
    "\n",
    "    model = pipeline_helper.train_model_no_eval( \n",
    "        model,\n",
    "        train_x_files,\n",
    "        train_y_files,\n",
    "        x_columns,\n",
    "        y_columns,\n",
    "        f'test_train_model_svr_scenario_{scenario}_fold_{fold}_fizz_{y_columns[0]}'\n",
    "    )\n",
    "    test_x_files = glob.glob(\n",
    "    f'/work/abslab/emognition_2023_challenge/data/scenario_{scenario}/fold_{fold}/test/physiology/*1hz.csv')\n",
    "\n",
    "    os.makedirs(f'/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_{scenario}/fold_{fold}/test/annotations/',exist_ok=True)\n",
    "    for test_x_file in test_x_files:\n",
    "        file_name_split = test_x_file.split('/')[-1].split('_')\n",
    "        sub, video = file_name_split[1],file_name_split[3]\n",
    "        annote_path = f'annotations/sub_{sub}_vid_{video}.csv'\n",
    "        annotation_file = f'/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_{scenario}/fold_{fold}/test/{annote_path}'\n",
    "        if os.path.exists(annotation_file):\n",
    "            annot_df = pd.read_csv(annotation_file)\n",
    "\n",
    "        else:\n",
    "            test_annotation_clean_path =test_x_file.split('phy')[0]+annote_path\n",
    "            annot_df = pd.read_csv(test_annotation_clean_path)\n",
    "        test_pred = model.predict(pd.read_csv(test_x_file)[x_columns].fillna(0).values)\n",
    "        upsampled_preds = upsample_to_answer_shape(test_pred,len(annot_df))\n",
    "        annot_df[y_columns[0]]=upsampled_preds\n",
    "        annot_df.to_csv(annotation_file,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5b5bd787-d349-4cc4-97bd-7c60630345a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>time</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.073239</td>\n",
       "      <td>4.878170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10050</td>\n",
       "      <td>5.071915</td>\n",
       "      <td>4.877917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10100</td>\n",
       "      <td>5.070435</td>\n",
       "      <td>4.879012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10150</td>\n",
       "      <td>5.068822</td>\n",
       "      <td>4.881414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10200</td>\n",
       "      <td>5.067103</td>\n",
       "      <td>4.885060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>596</td>\n",
       "      <td>596</td>\n",
       "      <td>39800</td>\n",
       "      <td>4.879948</td>\n",
       "      <td>5.135431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>597</td>\n",
       "      <td>597</td>\n",
       "      <td>39850</td>\n",
       "      <td>4.876341</td>\n",
       "      <td>5.137386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>598</td>\n",
       "      <td>598</td>\n",
       "      <td>39900</td>\n",
       "      <td>4.873217</td>\n",
       "      <td>5.138943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>599</td>\n",
       "      <td>599</td>\n",
       "      <td>39950</td>\n",
       "      <td>4.870546</td>\n",
       "      <td>5.140130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>600</td>\n",
       "      <td>600</td>\n",
       "      <td>40000</td>\n",
       "      <td>4.868288</td>\n",
       "      <td>5.140980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>601 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0.1  Unnamed: 0   time   valence   arousal\n",
       "0               0           0  10000  5.073239  4.878170\n",
       "1               1           1  10050  5.071915  4.877917\n",
       "2               2           2  10100  5.070435  4.879012\n",
       "3               3           3  10150  5.068822  4.881414\n",
       "4               4           4  10200  5.067103  4.885060\n",
       "..            ...         ...    ...       ...       ...\n",
       "596           596         596  39800  4.879948  5.135431\n",
       "597           597         597  39850  4.876341  5.137386\n",
       "598           598         598  39900  4.873217  5.138943\n",
       "599           599         599  39950  4.870546  5.140130\n",
       "600           600         600  40000  4.868288  5.140980\n",
       "\n",
       "[601 rows x 5 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsync -r /work/abslab/emognition_2023_challenge/data/scenario_2/fold_4/test/annotations/ /work/abslab/emognition_2023_challenge/results/test_submissions/scenario_2/fold_4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "addc1c37-02b7-4be7-a11f-5e5c89f2691e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcveigh.k/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcveigh.k/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcveigh.k/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcveigh.k/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcveigh.k/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcveigh.k/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcveigh.k/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcveigh.k/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcveigh.k/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcveigh.k/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "scenario = 2\n",
    "y_columns_list=[['arousal'],['valence']]\n",
    "for y_columns in y_columns_list:\n",
    "    for fold in range(5):\n",
    "        print(fold)\n",
    "        train_y_files = glob.glob(\n",
    "            f'/work/abslab/emognition_2023_challenge/data/scenario_{scenario}/fold_{fold}/train/annotations/*')\n",
    "        train_x_files = glob.glob(\n",
    "            f'/work/abslab/emognition_2023_challenge/data/scenario_{scenario}/fold_{fold}/train/physiology/*1hz_zscored.csv')\n",
    "        model = svm.SVR()\n",
    "        x_columns = [ \n",
    "            'PPG_Rate', \n",
    "            'ECG_Rate','ECG_Quality','PPG_Peak_Height',\n",
    "            'EDA_Clean', 'EDA_Tonic', 'EDA_Phasic', 'SCR_Onsets', 'SCR_Peaks','SCR_Height', 'SCR_Amplitude',\n",
    "            'RSP_Amplitude', 'RSP_Rate',\n",
    "            'corrugator_EMG_Clean',\n",
    "            'corrugator_EMG_Amplitude', 'corrugator_EMG_Activity',\n",
    "            'corrugator_EMG_Onsets', 'corrugator_EMG_Offsets', 'trapezius_EMG_Raw',\n",
    "            'trapezius_EMG_Clean', 'trapezius_EMG_Amplitude',\n",
    "            'trapezius_EMG_Activity', 'trapezius_EMG_Onsets',\n",
    "            'trapezius_EMG_Offsets', 'zygomaticus_EMG_Raw', 'zygomaticus_EMG_Clean',\n",
    "            'zygomaticus_EMG_Amplitude', 'zygomaticus_EMG_Activity',\n",
    "            'zygomaticus_EMG_Onsets', 'zygomaticus_EMG_Offsets',\n",
    "            'RSP_RVT', 'RSP_Phase', 'RSP_Phase_Completion',\n",
    "            'RSP_Symmetry_PeakTrough', 'RSP_Symmetry_RiseDecay','skt',\n",
    "            'sub_num',\n",
    "            'vid_num',\n",
    "        ]\n",
    "\n",
    "        model = pipeline_helper.train_model_no_eval( \n",
    "            model,\n",
    "            train_x_files,\n",
    "            train_y_files,\n",
    "            x_columns,\n",
    "            y_columns,\n",
    "            f'test_train_model_svr_scenario_{scenario}_fold_{fold}_fizz_{y_columns[0]}'\n",
    "        )\n",
    "        test_x_files = glob.glob(\n",
    "        f'/work/abslab/emognition_2023_challenge/data/scenario_{scenario}/fold_{fold}/test/physiology/*1hz.csv')\n",
    "        os.makedirs(f'/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_{scenario}/fold_{fold}/test/annotations/',exist_ok=True)\n",
    "        for test_x_file in test_x_files:\n",
    "            file_name_split = test_x_file.split('/')[-1].split('_')\n",
    "            sub, video = file_name_split[1],file_name_split[3]\n",
    "            annote_path = f'annotations/sub_{sub}_vid_{video}.csv'\n",
    "            annotation_file = f'/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_{scenario}/fold_{fold}/test/{annote_path}'\n",
    "            if os.path.exists(annotation_file):\n",
    "                annot_df = pd.read_csv(annotation_file)\n",
    "\n",
    "            else:\n",
    "                test_annotation_clean_path =test_x_file.split('phy')[0]+annote_path\n",
    "                annot_df = pd.read_csv(test_annotation_clean_path)\n",
    "            test_pred = model.predict(pd.read_csv(test_x_file)[x_columns].fillna(0).values)\n",
    "            upsampled_preds = upsample_to_answer_shape(test_pred,len(annot_df))\n",
    "            annot_df[y_columns[0]]=upsampled_preds\n",
    "            annot_df.to_csv(annotation_file,index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29cb4ff0-32f3-4ba8-9d16-c7bd064bc030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcveigh.k/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcveigh.k/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcveigh.k/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcveigh.k/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "scenario = 3\n",
    "y_columns=['valence']\n",
    "for fold in range(4):\n",
    "    print(fold)\n",
    "    train_y_files = glob.glob(\n",
    "        f'/work/abslab/emognition_2023_challenge/data/scenario_{scenario}/fold_{fold}/train/annotations/*')\n",
    "    train_x_files = glob.glob(\n",
    "        f'/work/abslab/emognition_2023_challenge/data/scenario_{scenario}/fold_{fold}/train/physiology/*1hz_zscored.csv')\n",
    "    model = svm.SVR()\n",
    "    x_columns = [ \n",
    "        'PPG_Rate', \n",
    "        'ECG_Rate','ECG_Quality','PPG_Peak_Height',\n",
    "        'EDA_Clean', 'EDA_Tonic', 'EDA_Phasic', 'SCR_Onsets', 'SCR_Peaks','SCR_Height', 'SCR_Amplitude',\n",
    "        'RSP_Amplitude', 'RSP_Rate',\n",
    "        'RSP_RVT', 'RSP_Phase', 'RSP_Phase_Completion',\n",
    "        'RSP_Symmetry_PeakTrough', 'RSP_Symmetry_RiseDecay','skt',\n",
    "        'sub_num',\n",
    "        #'vid_num',\n",
    "    ]\n",
    "\n",
    "    model = pipeline_helper.train_model_no_eval( \n",
    "        model,\n",
    "        train_x_files,\n",
    "        train_y_files,\n",
    "        x_columns,\n",
    "        y_columns,\n",
    "        f'test_train_model_svr_scenario_{scenario}_fold_{fold}_fizz_{y_columns[0]}'\n",
    "    )\n",
    "    test_x_files = glob.glob(\n",
    "    f'/work/abslab/emognition_2023_challenge/data/scenario_{scenario}/fold_{fold}/test/physiology/*1hz.csv')\n",
    "\n",
    "    os.makedirs(f'/work/abslab/emognition_2023_challenge/results/final_submission/scenario_{scenario}/fold_{fold}/test/annotations/',exist_ok=True)\n",
    "    for test_x_file in test_x_files:\n",
    "        file_name_split = test_x_file.split('/')[-1].split('_')\n",
    "        sub, video = file_name_split[1],file_name_split[3]\n",
    "        annote_path = f'annotations/sub_{sub}_vid_{video}.csv'\n",
    "        annotation_file = f'/work/abslab/emognition_2023_challenge/results/final_submission/scenario_{scenario}/fold_{fold}/test/{annote_path}'\n",
    "        if os.path.exists(annotation_file):\n",
    "            annot_df = pd.read_csv(annotation_file)\n",
    "\n",
    "        else:\n",
    "            test_annotation_clean_path =test_x_file.split('phy')[0]+annote_path\n",
    "            annot_df = pd.read_csv(test_annotation_clean_path)\n",
    "        test_pred = model.predict(pd.read_csv(test_x_file)[x_columns].fillna(0).values)\n",
    "        upsampled_preds = upsample_to_answer_shape(test_pred,len(annot_df))\n",
    "        annot_df[y_columns[0]]=upsampled_preds\n",
    "        annot_df.to_csv(annotation_file,index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa1ad8c-e9b4-4588-8eea-bcf7b16f93a0",
   "metadata": {},
   "source": [
    "# scenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23fd9977-6903-419e-8b58-c7758139adf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/abslab/emognition_2023_challenge/scripts/pipeline_helper.py:256: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model.fit(train_x, train_y)\n"
     ]
    }
   ],
   "source": [
    "scenario = 1\n",
    "y_columns=['arousal']\n",
    "train_y_files = glob.glob(\n",
    "    f'/work/abslab/emognition_2023_challenge/data/scenario_{scenario}/train/annotations/*.csv')\n",
    "train_x_files = glob.glob(\n",
    "    f'/work/abslab/emognition_2023_challenge/data/scenario_{scenario}/train/physiology/*1hz_zscored.csv')\n",
    "model = ensemble.RandomForestRegressor()\n",
    "x_columns = [ \n",
    "    'PPG_Rate', \n",
    "    'ECG_Rate','ECG_Quality','PPG_Peak_Height',\n",
    "    'EDA_Clean', 'EDA_Tonic', 'EDA_Phasic', 'SCR_Onsets', 'SCR_Peaks','SCR_Height', 'SCR_Amplitude',\n",
    "    'RSP_Amplitude', 'RSP_Rate',\n",
    "    'corrugator_EMG_Clean',\n",
    "    'corrugator_EMG_Amplitude', 'corrugator_EMG_Activity',\n",
    "    'corrugator_EMG_Onsets', 'corrugator_EMG_Offsets', 'trapezius_EMG_Raw',\n",
    "    'trapezius_EMG_Clean', 'trapezius_EMG_Amplitude',\n",
    "    'trapezius_EMG_Activity', 'trapezius_EMG_Onsets',\n",
    "    'trapezius_EMG_Offsets', 'zygomaticus_EMG_Raw', 'zygomaticus_EMG_Clean',\n",
    "    'zygomaticus_EMG_Amplitude', 'zygomaticus_EMG_Activity',\n",
    "    'zygomaticus_EMG_Onsets', 'zygomaticus_EMG_Offsets',\n",
    "    'RSP_RVT', 'RSP_Phase', 'RSP_Phase_Completion',\n",
    "    'RSP_Symmetry_PeakTrough', 'RSP_Symmetry_RiseDecay','skt',\n",
    "    'sub_num',\n",
    "    'vid_num',\n",
    "]\n",
    "\n",
    "model = pipeline_helper.train_model_no_eval( \n",
    "    model,\n",
    "    train_x_files,\n",
    "    train_y_files,\n",
    "    x_columns,\n",
    "    y_columns,\n",
    "    f'test_train_model_rf_scenario_{scenario}_fold_fizz_{y_columns[0]}'\n",
    ")\n",
    "test_x_files = glob.glob(\n",
    "f'/work/abslab/emognition_2023_challenge/data/scenario_{scenario}/test/physiology/*1hz.csv')\n",
    "\n",
    "os.makedirs(f'/work/abslab/emognition_2023_challenge/results/final_submission/results/scenario_{scenario}/test/annotations/',exist_ok=True)\n",
    "for test_x_file in test_x_files:\n",
    "    file_name_split = test_x_file.split('/')[-1].split('_')\n",
    "    sub, video = file_name_split[1],file_name_split[3]\n",
    "    annote_path = f'annotations/sub_{sub}_vid_{video}.csv'\n",
    "    annotation_file = f'/work/abslab/emognition_2023_challenge/results/final_submission/results/scenario_{scenario}/test/{annote_path}'\n",
    "    if os.path.exists(annotation_file):\n",
    "        annot_df = pd.read_csv(annotation_file)\n",
    "    \n",
    "    else:\n",
    "        test_annotation_clean_path =test_x_file.split('phy')[0]+annote_path\n",
    "        annot_df = pd.read_csv(test_annotation_clean_path)\n",
    "    test_pred = model.predict(pd.read_csv(test_x_file)[x_columns].fillna(0).values)\n",
    "    upsampled_preds = upsample_to_answer_shape(test_pred,len(annot_df))\n",
    "    annot_df[y_columns[0]]=upsampled_preds\n",
    "    annot_df.to_csv(annotation_file,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fd385ca-67c4-4f74-98ee-e5250884e574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/abslab/emognition_2023_challenge/scripts/pipeline_helper.py:256: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model.fit(train_x, train_y)\n"
     ]
    }
   ],
   "source": [
    "scenario = 1\n",
    "y_columns=['valence']\n",
    "train_y_files = glob.glob(\n",
    "    f'/work/abslab/emognition_2023_challenge/data/scenario_{scenario}/train/annotations/*.csv')\n",
    "train_x_files = glob.glob(\n",
    "    f'/work/abslab/emognition_2023_challenge/data/scenario_{scenario}/train/physiology/*1hz_zscored.csv')\n",
    "model = ensemble.RandomForestRegressor()\n",
    "x_columns = [ \n",
    "    'PPG_Rate', \n",
    "    'ECG_Rate','ECG_Quality','PPG_Peak_Height',\n",
    "    'EDA_Clean', 'EDA_Tonic', 'EDA_Phasic', 'SCR_Onsets', 'SCR_Peaks','SCR_Height', 'SCR_Amplitude',\n",
    "    'RSP_Amplitude', 'RSP_Rate',\n",
    "    'corrugator_EMG_Clean',\n",
    "    'corrugator_EMG_Amplitude', 'corrugator_EMG_Activity',\n",
    "    'corrugator_EMG_Onsets', 'corrugator_EMG_Offsets', 'trapezius_EMG_Raw',\n",
    "    'trapezius_EMG_Clean', 'trapezius_EMG_Amplitude',\n",
    "    'trapezius_EMG_Activity', 'trapezius_EMG_Onsets',\n",
    "    'trapezius_EMG_Offsets', 'zygomaticus_EMG_Raw', 'zygomaticus_EMG_Clean',\n",
    "    'zygomaticus_EMG_Amplitude', 'zygomaticus_EMG_Activity',\n",
    "    'zygomaticus_EMG_Onsets', 'zygomaticus_EMG_Offsets',\n",
    "    'RSP_RVT', 'RSP_Phase', 'RSP_Phase_Completion',\n",
    "    'RSP_Symmetry_PeakTrough', 'RSP_Symmetry_RiseDecay','skt',\n",
    "    'sub_num',\n",
    "    'vid_num',\n",
    "]\n",
    "\n",
    "model = pipeline_helper.train_model_no_eval( \n",
    "    model,\n",
    "    train_x_files,\n",
    "    train_y_files,\n",
    "    x_columns,\n",
    "    y_columns,\n",
    "    f'test_train_model_rf_scenario_{scenario}_fold_fizz_{y_columns[0]}'\n",
    ")\n",
    "test_x_files = glob.glob(\n",
    "f'/work/abslab/emognition_2023_challenge/data/scenario_{scenario}/test/physiology/*1hz.csv')\n",
    "\n",
    "os.makedirs(f'/work/abslab/emognition_2023_challenge/results/final_submission/results/scenario_{scenario}/test/annotations/',exist_ok=True)\n",
    "for test_x_file in test_x_files:\n",
    "    file_name_split = test_x_file.split('/')[-1].split('_')\n",
    "    sub, video = file_name_split[1],file_name_split[3]\n",
    "    annote_path = f'annotations/sub_{sub}_vid_{video}.csv'\n",
    "    annotation_file = f'/work/abslab/emognition_2023_challenge/results/final_submission/results/scenario_{scenario}/test/{annote_path}'\n",
    "    if os.path.exists(annotation_file):\n",
    "        annot_df = pd.read_csv(annotation_file)\n",
    "    \n",
    "    else:\n",
    "        test_annotation_clean_path =test_x_file.split('phy')[0]+annote_path\n",
    "        annot_df = pd.read_csv(test_annotation_clean_path)\n",
    "    test_pred = model.predict(pd.read_csv(test_x_file)[x_columns].fillna(0).values)\n",
    "    upsampled_preds = upsample_to_answer_shape(test_pred,len(annot_df))\n",
    "    annot_df[y_columns[0]]=upsampled_preds\n",
    "    annot_df.to_csv(annotation_file,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f82406e-77c5-438f-ac70-a126dd5e019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario = 1\n",
    "# test_x_files = glob.glob(\n",
    "# f'/work/abslab/emognition_2023_challenge/data/scenario_{scenario}/test/physiology/*1hz.csv')\n",
    "\n",
    "# for test_x_file in test_x_files:\n",
    "#     file_name_split = test_x_file.split('/')[-1].split('_')\n",
    "#     sub, video = file_name_split[1],file_name_split[3]\n",
    "#     annote_path = f'annotations/sub_{sub}_vid_{video}.csv'\n",
    "#     annotation_file = f'/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_{scenario}/test/{annote_path}'\n",
    "#     if os.path.exists(annotation_file):\n",
    "#         annot_df = pd.read_csv(annotation_file)\n",
    "    \n",
    "#     else:\n",
    "#         test_annotation_clean_path =test_x_file.split('phy')[0]+annote_path\n",
    "#         annot_df = pd.read_csv(test_annotation_clean_path)\n",
    "#     test_pred = model.predict(pd.read_csv(test_x_file)[x_columns].fillna(0).values)\n",
    "#     upsampled_preds = upsample_to_answer_shape(test_pred,len(annot_df))\n",
    "#     annot_df[y_columns[0]]=upsampled_preds\n",
    "#     annot_df.to_csv(annotation_file,index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bd14cc-7764-4f38-b151-5f5464a6b01c",
   "metadata": {},
   "source": [
    "# scenario 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "652ec124-9b05-4d6a-96fa-e840d7b164ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcveigh.k/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcveigh.k/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "scenario = 4\n",
    "y_columns=['valence']\n",
    "for fold in range(2):\n",
    "    print(fold)\n",
    "    train_y_files = glob.glob(\n",
    "        f'/work/abslab/emognition_2023_challenge/data/scenario_{scenario}/fold_{fold}/train/annotations/*')\n",
    "    train_x_files = glob.glob(\n",
    "        f'/work/abslab/emognition_2023_challenge/data/scenario_{scenario}/fold_{fold}/train/physiology/*1hz_zscored.csv')\n",
    "    model = svm.SVR()\n",
    "    x_columns = [ \n",
    "        'PPG_Rate', \n",
    "        'ECG_Rate','ECG_Quality','PPG_Peak_Height',\n",
    "        'EDA_Clean', 'EDA_Tonic', 'EDA_Phasic', 'SCR_Onsets', 'SCR_Peaks','SCR_Height', 'SCR_Amplitude',\n",
    "        'RSP_Amplitude', 'RSP_Rate',\n",
    "        'RSP_RVT', 'RSP_Phase', 'RSP_Phase_Completion',\n",
    "        'RSP_Symmetry_PeakTrough', 'RSP_Symmetry_RiseDecay','skt',\n",
    "        'sub_num',\n",
    "        #'vid_num',\n",
    "    ]\n",
    "\n",
    "    model = pipeline_helper.train_model_no_eval( \n",
    "        model,\n",
    "        train_x_files,\n",
    "        train_y_files,\n",
    "        x_columns,\n",
    "        y_columns,\n",
    "        f'test_train_model_svr_scenario_{scenario}_fold_{fold}_fizz_{y_columns[0]}'\n",
    "    )\n",
    "    test_x_files = glob.glob(\n",
    "    f'/work/abslab/emognition_2023_challenge/data/scenario_{scenario}/fold_{fold}/test/physiology/*1hz.csv')\n",
    "\n",
    "    os.makedirs(f'/work/abslab/emognition_2023_challenge/results/final_submission/results/scenario_{scenario}/fold_{fold}/test/annotations/',exist_ok=True)\n",
    "    for test_x_file in test_x_files:\n",
    "        file_name_split = test_x_file.split('/')[-1].split('_')\n",
    "        sub, video = file_name_split[1],file_name_split[3]\n",
    "        annote_path = f'annotations/sub_{sub}_vid_{video}.csv'\n",
    "        annotation_file = f'/work/abslab/emognition_2023_challenge/results/final_submission/results/scenario_{scenario}/fold_{fold}/test/{annote_path}'\n",
    "        if os.path.exists(annotation_file):\n",
    "            annot_df = pd.read_csv(annotation_file)\n",
    "\n",
    "        else:\n",
    "            test_annotation_clean_path =test_x_file.split('phy')[0]+annote_path\n",
    "            annot_df = pd.read_csv(test_annotation_clean_path)\n",
    "        test_pred = model.predict(pd.read_csv(test_x_file)[x_columns].fillna(0).values)\n",
    "        upsampled_preds = upsample_to_answer_shape(test_pred,len(annot_df))\n",
    "        annot_df[y_columns[0]]=upsampled_preds\n",
    "        annot_df.to_csv(annotation_file,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4ee6b8a-63bc-4e46-a1c7-9205a0911715",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_11_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_11_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_11_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_11_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_11_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_11_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_11_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_11_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_12_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_12_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_12_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_12_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_12_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_12_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_12_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_12_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_13_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_13_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_13_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_13_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_13_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_13_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_13_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_13_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_14_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_14_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_14_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_14_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_14_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_14_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_14_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_14_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_17_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_17_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_17_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_17_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_17_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_17_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_17_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_17_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_18_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_18_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_18_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_18_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_18_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_18_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_18_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_18_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_19_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_19_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_19_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_19_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_19_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_19_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_19_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_19_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_1_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_1_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_1_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_1_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_1_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_1_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_1_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_1_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_20_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_20_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_20_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_20_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_20_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_20_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_20_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_20_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_22_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_22_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_22_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_22_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_22_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_22_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_22_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_22_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_26_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_26_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_26_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_26_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_26_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_26_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_26_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_26_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_28_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_28_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_28_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_28_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_28_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_28_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_28_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_28_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_29_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_29_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_29_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_29_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_29_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_29_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_29_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_29_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_30_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_30_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_30_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_30_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_30_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_30_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_30_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_30_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_31_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_31_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_31_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_31_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_31_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_31_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_31_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_31_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_32_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_32_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_32_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_32_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_32_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_32_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_32_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_32_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_33_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_33_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_33_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_33_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_33_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_33_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_33_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_33_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_34_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_34_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_34_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_34_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_34_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_34_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_34_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_34_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_35_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_35_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_35_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_35_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_35_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_35_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_35_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_35_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_36_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_36_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_36_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_36_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_36_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_36_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_36_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_36_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_37_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_37_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_37_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_37_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_37_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_37_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_37_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_37_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_38_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_38_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_38_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_38_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_38_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_38_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_38_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_38_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_41_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_41_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_41_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_41_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_41_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_41_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_41_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_41_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_43_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_43_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_43_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_43_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_43_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_43_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_43_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_43_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_45_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_45_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_45_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_45_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_45_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_45_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_45_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_45_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_4_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_4_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_4_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_4_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_4_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_4_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_4_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_4_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_6_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_6_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_6_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_6_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_6_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_6_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_6_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_6_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_7_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_7_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_7_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_7_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_7_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_7_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_7_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_7_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_8_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_8_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_8_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_8_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_8_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_8_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_8_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_8_vid_9.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_9_vid_1.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_9_vid_10.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_9_vid_11.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_9_vid_13.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_9_vid_14.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_9_vid_18.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_9_vid_20.csv',\n",
       " '/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/test/annotations/sub_9_vid_9.csv']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abe915b3-d3f2-4c9c-b197-976bd1fdc789",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in glob.glob('/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_1/*/*/*'):\n",
    "    df = pd.read_csv(path)\n",
    "    if sum(df.isna()['arousal']*1)>0:\n",
    "        print(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81e9db48-9d5d-4364-8074-1534752c8c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in glob.glob('/work/abslab/emognition_2023_challenge/results/test_submissions/scenario_[2-4]/*/*/*/*'):\n",
    "    df = pd.read_csv(path)\n",
    "    if sum(df.isna()['arousal']*1)>0:\n",
    "        print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0022ff61-3cb5-4ffc-be5f-327bbc635d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
